{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlqVBEfLBOF_"
   },
   "source": [
    "**SOW-MKI49: Neural Information Processing Systems**  \n",
    "*Weeks 2 and 3: Assignment (200 points + 20 bonus points + 1 bonus point for each bug you find and another bonus point if you debug it and before you ask, no, typos unfortunately are not considered bugs - first come, first served)*  \n",
    "Author: Umut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rT8BaKk5CpeB"
   },
   "outputs": [],
   "source": [
    "# Group number: 25\n",
    "# Felicity Reddel, s4830717\n",
    "# Max Reddel, s4830709\n",
    "# Johan van den Heuvel, s4770528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "00iIAIv37Del"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "data_directory = 'Data'\n",
    "#data_directory = r'/Users/DieGepardin/Documents/DATA' # Make a directory to store the data and enter it here.\n",
    "                    # We will be using a smaller dataset (LFW) than the one used in the paper (CelebA) for computational resource considerations.\n",
    "                    # Download it from http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz.\n",
    "device = -1\n",
    "epochs = 100\n",
    "lambda_ = {'feature': 1., 'pixel': 1., 'total_variation': 1e-5}\n",
    "model_directory = 'Model' # Make a directory to store the models and enter it here. Move Vgg4Layers.npz to the model directory.\n",
    "outsize = (96, 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86zZctPu1K2M"
   },
   "source": [
    "**Packages (10 points)**  \n",
    "In this cell, you will import the required packages.  \n",
    "*Tasks*   \n",
    "- (1) It is always good practice to first think about the big picture and not rush into writing code before clearly knowing everything that you will have to do so as to avoid future complications. Therefore, your first task is to study the skeleton code and come up with a plan of how to proceed. (**0 points**)\n",
    "- (2) However, I agree that doing so is arguably the most boring part of coding, and you rather skip it. To help you to resist the temptation of skipping going through the skeleton code, I have removed the import statements. Your second task is to Identify the required packages and import them. Note that if you are using Python 2.7, you should import print from the future. (**10 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBiJw5pV030o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda3/envs/python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# (2) start\n",
    "\"\"\"\n",
    "In the lecture it was told we can use skicit-image for elementary image processing algorithms, e.g. resizing.\n",
    "\"\"\"\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer import Link, Chain, ChainList, Function, iterators\n",
    "import chainer.links as L \n",
    "import chainer.functions as F\n",
    "from chainer.dataset import DatasetMixin, concat_examples\n",
    "from chainer.serializers import load_npz, save_npz\n",
    "from chainer.optimizers import Adam\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "# (2) end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOhjvsOx9lJY"
   },
   "source": [
    "**Preprocessing functions (10 points + 5 bonus points)** (taken from https://github.com/mbeyeler/opencv-python-blueprints)  \n",
    "In the following cell, you will implement some of the preprocessing functions. The rest of the preprocessing steps have already been applied to the data.  \n",
    "*Tasks*\n",
    "- (1) Implement the resizing operation. That is, you should extract the data, resize each portrait to 96 pixels x 96 pixels and save them to the data directory as JPG. (**10 points **)\n",
    "- (2) The pencil sketch class implements the sketch effect in a simpler way than the one mentioned in the lecture. Explain how/why the used operations (blur and divide) convert portraits to sketches, and how it differs from that which was mentioned in the lecture? (**5 bonus points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY4lbpLK9kp4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/anaconda3/envs/python3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/johan/anaconda3/envs/python3/lib/python3.6/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# (1) start\n",
    "tar = tarfile.open(data_directory + \"/\" + \"lfw-deepfunneled.tgz\", \"r\")\n",
    "for filename in tar.getnames():\n",
    "    \"\"\"\n",
    "    Tarfile uses working directory, but skimage.io uses home directory. \n",
    "    So for skimage.io we need to append os.getcwd() i.e. the path to the working directory. \n",
    "    \"\"\"\n",
    "    tar.extract(filename, data_directory)\n",
    "    imname = os.path.join(os.getcwd(), data_directory + \"/\" + filename) \n",
    "    im = io.imread(imname)\n",
    "    im_resized = resize(im,outsize)\n",
    "    io.imsave(imname, im_resized)    \n",
    "print(\"done\")\n",
    "# (1) end\n",
    "\n",
    "class PencilSketch:\n",
    "    \"\"\"Pencil sketch effect\n",
    "        A class that applies a pencil sketch effect to an image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension):\n",
    "        \"\"\"Initialize parameters\n",
    "            :param (width, height): Image size.\n",
    "        \"\"\"\n",
    "        self.width, self.height = dimension\n",
    "        \n",
    "\n",
    "        # try to open background canvas (if it exists)\n",
    "        #self.canvas = cv2.imread(bg_gray, cv2.CV_8UC1)\n",
    "        #if self.canvas is not None:\n",
    "        #    self.canvas = cv2.resize(self.canvas, (self.width, self.height))\n",
    "\n",
    "    def render(self, img_rgb):\n",
    "        \"\"\"Applies pencil sketch effect to an RGB image\n",
    "            :param img_rgb: RGB image to be processed\n",
    "            :returns: Processed RGB image\n",
    "        \"\"\"\n",
    "        img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        img_blur = cv2.GaussianBlur(img_gray, (21, 21), 0, 0)\n",
    "        img_blend = cv2.divide(img_gray, img_blur, scale=256)\n",
    "\n",
    "        # return cv2.cvtColor(img_blend, cv2.COLOR_GRAY2RGB)\n",
    "        return img_blend\n",
    "\n",
    "def pencil_sketch(img_rgb):\n",
    "    pencilSketch = PencilSketch((img_rgb.shape[1], img_rgb.shape[0]))\n",
    "\n",
    "    return pencilSketch.render(img_rgb)\n",
    "\n",
    "# (2) Write your answer here.\n",
    "\"\"\" We are working with a greyed version of the original image and with a blurred version of the greyed image.\n",
    "The blurred image is still very close to the greyed version. In brighter areas with less contrast, the dividing of\n",
    "one version by the other results in (almost) totally white pixels. In darker areas with already a bit more contrast\n",
    "(e.g. at borders and edges), the dividing operation does not change the image much. \n",
    "The extreme case: If totally black, it doesn't change at all.\n",
    "The consequence is that we end up with a brighter and more contrast-full 'sketch'of the original color image.\n",
    "So the small details go away because of the blur and the edges remain because of the contrast.\n",
    "\n",
    "This seems a lot faster and easier to compute than edge detection (the method that was mentioned in the lecture).\n",
    "Edge detection requires finding locations of abrupt changes in an image (e.g. brightness, depth, material structure).\n",
    "This process often makes use of more extensive computations like first- & second-order derivations \n",
    "and non-linear differential equations. \n",
    "\"\"\"\n",
    "\n",
    "def preprocess(img):\n",
    "    if img.mode == 'L':\n",
    "        return np.rollaxis(np.asarray(img, 'float32')[..., None], 2)\n",
    "    else:\n",
    "        return np.rollaxis(np.asarray(img, 'float32')[..., ::-1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovQMUuo_7D2k"
   },
   "source": [
    "**Data class**  \n",
    "The following cell defines the data class. It is used to manage the data (loading, etc.). *You do not have to make any changes to the code.*  \n",
    "*Task*\n",
    "- (1) Study the code and refer to the chainer documentation if anything is unclear. You will be expected to write similar code by yourself in the coming practicals. (**0 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-OF39paH6wff"
   },
   "outputs": [],
   "source": [
    "class Dataset(DatasetMixin):\n",
    "    def __init__(self, data_files):\n",
    "        self.data_files = data_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        t = np.asarray(Image.open(self.data_files[i]).convert('RGB').resize((96, 96), Image.LANCZOS), 'f').transpose(2, 0, 1)\n",
    "        x = pencil_sketch(np.asarray(Image.open(self.data_files[i]).convert('RGB').resize((96, 96), Image.LANCZOS), 'f'))[None]\n",
    "\n",
    "        return t, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUjEFdDD6xBq"
   },
   "source": [
    "**Model classes (45 points)**  \n",
    "In the following cell you will implement the model classes.\n",
    "*Tasks*   \n",
    "- (1) Implement the layers of the model by filling in the missing code. (**20 points**)\n",
    "- (2) Reimplement the model as a ChainList instead of a Chain. (**5 points**)\n",
    "- (3) Implement the forward pass of the residual block by filling in the missing code. (**20 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nafY2Wgx6QLt"
   },
   "outputs": [],
   "source": [
    "class Model(Chain):\n",
    "    def __init__(self, in_channels, outsize):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            # (1) start\n",
    "            self.convolution2D_0 = L.Convolution2D(in_channels, 32, ksize=9, stride=1, pad=4) # potentially check None & in_channels\n",
    "            self.batchNormalization_0 = L.BatchNormalization(32)\n",
    "            self.convolution2D_1 = L.Convolution2D(None, 64, ksize=3, stride=2, pad=1)\n",
    "            self.batchNormalization_1 = L.BatchNormalization(64)\n",
    "            self.convolution2D_2 = L.Convolution2D(None, 128, ksize=3, stride=2, pad=1)\n",
    "            self.batchNormalization_2 = L.BatchNormalization(128)\n",
    "            self.residualBlock_3 = ResidualBlock(128, 128)\n",
    "            self.residualBlock_4 = ResidualBlock(128, 128)\n",
    "            self.residualBlock_5 = ResidualBlock(128, 128)\n",
    "            self.residualBlock_6 = ResidualBlock(128, 128)\n",
    "            self.residualBlock_7 = ResidualBlock(128, 128)\n",
    "            self.deconvolution2D_8 = L.Deconvolution2D(None, 64, 3, 2, 1, nobias=True, outsize=outsize)\n",
    "            self.batchNormalization_8 = L.BatchNormalization(64)\n",
    "            # (1) end\n",
    "            self.deconvolution2D_9 = L.Deconvolution2D(64, 32, 3, 2, 1, True, outsize) # self. needed!\n",
    "            self.batchNormalization_9 = L.BatchNormalization(32) \n",
    "            self.convolution2D_10 = L.Convolution2D(32, 3, 9, pad = 4, nobias = True)\n",
    "            self.batchNormalization_10 = L.BatchNormalization(3)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.outsize = outsize\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "        h = self.convolution2D_0(x)\n",
    "        h = self.batchNormalization_0(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h)\n",
    "        h = self.batchNormalization_1(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_2(h)\n",
    "        h = self.batchNormalization_2(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.residualBlock_3(h, finetune)\n",
    "        h = self.residualBlock_4(h, finetune)\n",
    "        h = self.residualBlock_5(h, finetune)\n",
    "        h = self.residualBlock_6(h, finetune)\n",
    "        h = self.residualBlock_7(h, finetune)\n",
    "        h = self.deconvolution2D_8(h)\n",
    "        h = self.batchNormalization_8(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.deconvolution2D_9(h)\n",
    "        h = self.batchNormalization_9(h, finetune)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_10(h)\n",
    "        h = self.batchNormalization_10(h, finetune)\n",
    "        y = 127.5 * F.tanh(h) + 127.5\n",
    "\n",
    "        return y\n",
    "\n",
    "class Model(ChainList):\n",
    "    # (2) start\n",
    "    def __init__(self, in_channels, outsize):\n",
    "        super(Model, self).__init__()\n",
    "        self.add_link(L.Convolution2D(in_channels, out_channels=32, ksize=9, stride=1, pad=4))\n",
    "        self.add_link(L.BatchNormalization(size=32))\n",
    "        \n",
    "        self.add_link(L.Convolution2D(in_channels=32, out_channels=64, ksize=3, stride=2, pad=1))\n",
    "        self.add_link(L.BatchNormalization(size=64))\n",
    "        \n",
    "        self.add_link(L.Convolution2D(in_channels=64, out_channels=128, ksize=3, stride=2, pad=1))\n",
    "        self.add_link(L.BatchNormalization(size=128))\n",
    "        \n",
    "        self.add_link(ResidualBlock(in_channels=128, out_channels=128))\n",
    "        self.add_link(ResidualBlock(in_channels=128, out_channels=128))\n",
    "        self.add_link(ResidualBlock(in_channels=128, out_channels=128))\n",
    "        self.add_link(ResidualBlock(in_channels=128, out_channels=128))\n",
    "        self.add_link(ResidualBlock(in_channels=128, out_channels=128))\n",
    "    \n",
    "        self.add_link(L.Deconvolution2D(in_channels=128, out_channels=64, ksize=3, stride=2, pad=1))\n",
    "        self.add_link(L.BatchNormalization(size=64))\n",
    "    \n",
    "        self.add_link(L.Deconvolution2D(64, 32, 3, 2, 1, True, outsize))\n",
    "        self.add_link(L.BatchNormalization(32))\n",
    "    \n",
    "        self.add_link(L.Convolution2D(32, 3, 9, pad = 4, nobias = True))\n",
    "        self.add_link(L.BatchNormalization(3))\n",
    "    \n",
    "        self.train = True\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        print(x)\n",
    "        print(x.shape)\n",
    "        h = self[0](x)\n",
    "        h = self[1](h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self[2](h)\n",
    "        h = self[3](h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self[4](h)\n",
    "        h = self[5](h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self[6](h)\n",
    "        h = self[7](h)\n",
    "        h = self[8](h)\n",
    "        h = self[9](h)\n",
    "        h = self[10](h)\n",
    "        \n",
    "        h = self[11](h)\n",
    "        h = self[12](h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        h = self[13](h)\n",
    "        h = self[14](h)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self[15](h)\n",
    "        h = self[16](h)        \n",
    "        y = 127.5 * F.tanh(h) + 127.5\n",
    "\n",
    "        return y\n",
    "    # (2) end\n",
    "\n",
    "class ResidualBlock(Chain):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.batchNormalization_0 = L.BatchNormalization(size=out_channels)\n",
    "            self.batchNormalization_1 = L.BatchNormalization(size=out_channels)\n",
    "            self.convolution2D_0 = L.Convolution2D( in_channels, out_channels, 3, pad = 1, nobias = True)\n",
    "            self.convolution2D_1 = L.Convolution2D(out_channels, out_channels, 3, pad = 1, nobias = True)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def __call__(self, x, finetune = False):\n",
    "        # (3) start\n",
    "        h = self.convolution2D_0(x)\n",
    "        h = self.batchNormalization_0(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h)\n",
    "        h = self.batchNormalization_1(h)\n",
    "        y = F.relu(h+x)\n",
    "        # (3) start\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euDbOQWT1UA8"
   },
   "source": [
    "**Loss classes (45 points)**  \n",
    "In the following cell, you will implement the loss classes.  \n",
    "*Tasks*  \n",
    "- (1) You are provided with a custom VGG-16 implementation. How does it differ than the original implementation? Why can we get away with using the simpler implementation? (**5 points**)\n",
    "- (2) Implement the missing convolution layer of the total variation loss by filling in the missing code. (**10 points**)\n",
    "- (3) Implement the forward pass of the total variation loss by filling in the missing code. (**10 points**)\n",
    "- (4) Implement the feature loss component in the forward pass of the loss function by filling in the missing code. (**10 points**)\n",
    "- (5) Explain why the loss components are scaled. (**5 points**)\n",
    "- (6) Explain why the target features are extracted in test mode. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DqHGhS_1M_x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We need the target features to be able to arrive at the feature_loss component. \\nWithout the features of the target images, we cannot judge how well the network is doing in this regard.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vgg4Layers(Chain):\n",
    "    def __init__(self):\n",
    "        super(Vgg4Layers, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            convolution2D_0 = L.Convolution2D(  3,  64, 3, pad = 1)\n",
    "            convolution2D_1 = L.Convolution2D( 64,  64, 3, pad = 1)\n",
    "            convolution2D_2 = L.Convolution2D( 64, 128, 3, pad = 1)\n",
    "            convolution2D_3 = L.Convolution2D(128, 128, 3, pad = 1)\n",
    "\n",
    "#             self.add_persistent('mean', np.array([[[[103.939]], [[116.779]], [[123.68]]]],'float32')) # hardcoded rgb mean\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = x - F.broadcast_to(self.mean, x.shape) # TODO: How is this subtracted if x is greyscale and subtract rgb means?\n",
    "        h = self.convolution2D_0(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = self.convolution2D_2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convolution2D_3(h)\n",
    "        y = F.relu(h)\n",
    "        return y\n",
    "\n",
    "class TotalVariationLoss(Chain):\n",
    "    def __init__(self):\n",
    "        super(TotalVariationLoss, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            convolution2D_0 = L.Convolution2D(3, 1, 2, nobias = True, initialW = np.array([3 * [[[-1], [1]]]], 'float32'))\n",
    "            # (2) start\n",
    "            convolution2D_1 = L.Convolution2D(3, 1, 2, nobias = True, initialW = np.array([3 * [[[-1, 1]]]], 'float32'))\n",
    "            # (2) end   ##### TODO: CHECK WHETHER ROW VECTOR JUST ABOVE AND WHETHER GOOD LIKE THIS\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # (3) start\n",
    "        y = np.sqrt(convolution2D_0**2 + convolution2D_1**2)\n",
    "        # (3) end\n",
    "\n",
    "        return y\n",
    "\n",
    "class LossFunction(object):\n",
    "    def __init__(self, lambda_):\n",
    "        self.totalVariationLoss = TotalVariationLoss()\n",
    "        self.vgg4Layers         = Vgg4Layers()\n",
    "\n",
    "        def __call__(t, y):            \n",
    "            with chainer.using_config('train', False):\n",
    "                t_ = self.vgg4Layers(t)\n",
    "\n",
    "            # (4) start        \n",
    "            y_ = self.vgg4Layers(y)\n",
    "            feature_loss = lambda_['feature'] * F.mean_squared_error(t_, y_)\n",
    "            # (4) end\n",
    "            pixel_loss = lambda_['pixel'] * F.mean_squared_error(t , y)\n",
    "            total_variation_loss = lambda_['total_variation'] * self.totalVariationLoss(t, y)\n",
    "            loss = feature_loss + pixel_loss + total_variation_loss\n",
    "\n",
    "            return loss\n",
    "\n",
    "# (1) Write your answer here.      ####### TODO: CHECK (1), (5), (6) \n",
    "\"\"\"It differs in the size/depth of the vgg network. The vgg16 is way deeper than the vgg4.\n",
    "We can get away with the more shallow network because we only need rather simple features such as lines and edges,\n",
    "which are encoded in rather more shallow layers. \n",
    "They need to be able to encode and recognize more complex features to classify complex objects correctly.\n",
    "We are not in need of that as we don't need to identify complex features but only have the simple features.\"\"\"\n",
    "# (5) Write your answer here.\n",
    "\"\"\"We scale the individual parts of the loss to shift the emphasis between them. It turns out that each of the \n",
    "parts is relevant. At the same time, a more fitting loss function can be achieved by adapting their relative weight\n",
    "of the overall loss.\"\"\"\n",
    "# (6) Write your answer here.\n",
    "\"\"\"We need the target features to be able to arrive at the feature_loss component. \n",
    "Without the features of the target images, we cannot judge how well the network is doing in this regard.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nGcCNEy8p3g"
   },
   "source": [
    "**Initialization (10 points)**  \n",
    "The following cell initializes the loss function, the loss history, the model, the optimizer, the datasets and the iterators. *You do not have to make any changes to the code.*  \n",
    "*Tasks*\n",
    "- (1) Study the code and refer to the chainer docuimentation if anything is unclear. You will be expected to write similar code by yourself in the coming practicals. (**0 points**)  \n",
    "- (2) What are the boolean arguments that are passed to the SerialIterator class? (**5 points**)  \n",
    "- (3) Why is it false for the training iterator but not for other iterators? In other words, what would happen if we were to set it to false for the training iterator and true for the other iterators? (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hAa-KI4W-3Mm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13233\n"
     ]
    }
   ],
   "source": [
    "lossFunction = LossFunction(lambda_)\n",
    "load_npz('{:s}/Vgg4Layers.npz'.format(model_directory), lossFunction.vgg4Layers)\n",
    "loss_history = {'training': [], 'validation': []}\n",
    "model = Model(3, outsize) if device < 0 else Model(3, outsize).to_gpu(device) # Specifying to which GPU\n",
    "optimizer = Adam()\n",
    "\n",
    "optimizer.setup(model)\n",
    "\n",
    "data_file = sorted(glob('{}/lfw-deepfunneled/*/*.jpg'.format(data_directory)))\n",
    "training_set = Dataset(data_file[:int(.64 * len(data_file))])                              # 64% for training\n",
    "validation_set = Dataset(data_file[int(.64 * len(data_file)) : int(.8 * len(data_file))])  # 16% for validation\n",
    "test_set = Dataset(data_file[:int(.8 * len(data_file))])                                  # 20% for testing\n",
    "training_iterator = iterators.SerialIterator(training_set, batch_size, repeat=False, shuffle=True) # Shuffle because: more randomness during training? Using some examples multiple times -> generalizing better? \n",
    "test_iterator = iterators.SerialIterator(test_set, batch_size, repeat=False, shuffle=False)  # No shuffle to really test everything once? To have a stable evaluation metric\n",
    "validation_iterator = iterators.SerialIterator(validation_set , batch_size, repeat=False, shuffle=False)# No shuffle to really test everything once? To have a stable evaluation metric\n",
    "\n",
    "\n",
    "# (2) Write your answer here.\n",
    "# repeat and shuffle\n",
    "# (3) Write your answer here.\n",
    "# Training iterator: Shuffle because: more randomness during training? Using some examples multiple times -> generalizing better? \n",
    "# Validation & Test iterators: No shuffle to really test everything once? To have a stable evaluation metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAKIEbqPFzsc"
   },
   "source": [
    "**Training and validation (20 points)**  \n",
    "In the following cell, you will train and validate your model.\n",
    "*Tasks*   \n",
    "- (1) Implement training loss estimation, backprop and parameter update. (**10 points**)\n",
    "- (2) Implement validation loss history (**5 points**)\n",
    "- (3) Implement model serialization  (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F-pOSKTw0tcK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[260.9673    280.3695    306.5391    ... 366.34482   360.4976\n",
      "    355.1926   ]\n",
      "   [270.26938   278.49463   285.7807    ... 352.0802    337.77356\n",
      "    324.76108  ]\n",
      "   [274.9444    280.70343   274.6822    ... 377.45776   379.0803\n",
      "    375.98505  ]\n",
      "   ...\n",
      "   [261.40912   265.73828   264.92413   ... 256.6651    257.96747\n",
      "    259.16705  ]\n",
      "   [259.9844    264.88843   265.28058   ... 255.23793   256.512\n",
      "    257.70053  ]\n",
      "   [258.44193   263.61472   265.20663   ... 254.00366   255.26787\n",
      "    256.45267  ]]]\n",
      "\n",
      "\n",
      " [[[  4.1287303  38.414413    8.685514  ...   3.8301165   3.8555653\n",
      "      3.8642833]\n",
      "   [  5.199677    6.1451273  34.355465  ...  18.17487    22.860928\n",
      "     27.491606 ]\n",
      "   [  7.27892     3.044907  145.59181   ...   7.080054    7.1179748\n",
      "      7.1309457]\n",
      "   ...\n",
      "   [351.0735    353.17633   359.4992    ... 231.32855   113.999275\n",
      "     49.584785 ]\n",
      "   [ 47.672115   60.00656    73.40323   ... 208.93207   116.4517\n",
      "     50.66586  ]\n",
      "   [ 24.193727   48.854034   49.799114  ... 210.4014    117.306656\n",
      "     51.042976 ]]]\n",
      "\n",
      "\n",
      " [[[ 11.124436    2.0153005   2.02398   ...  12.378247    3.4075353\n",
      "      3.4875085]\n",
      "   [  1.9247609   1.9277222   4.8591723 ...   7.255512   33.69189\n",
      "      4.6722727]\n",
      "   [ 18.08129    18.445656   10.3877325 ...   7.296158    4.576039\n",
      "     24.066786 ]\n",
      "   ...\n",
      "   [  2.2672722  49.667027    3.894736  ... 298.28262   285.80777\n",
      "    311.4688   ]\n",
      "   [ 14.412692   31.063448    1.6849923 ...  23.267145   27.187582\n",
      "      5.6375523]\n",
      "   [  6.751978    2.1052468  15.843861  ...   6.6781898   2.6772656\n",
      "      9.464116 ]]]\n",
      "\n",
      "\n",
      " [[[  0.          0.          0.        ...   0.          0.\n",
      "      0.       ]\n",
      "   [  0.          0.          0.        ...   0.          0.\n",
      "      0.       ]\n",
      "   [  0.          0.          0.        ...   0.          0.\n",
      "      0.       ]\n",
      "   ...\n",
      "   [340.40283   342.01495   297.638     ... 223.8526    242.64813\n",
      "    257.59564  ]\n",
      "   [357.50308   354.68466   174.3055    ... 229.89494   219.96008\n",
      "    216.20604  ]\n",
      "   [259.33347   306.24554    64.50089   ... 223.67635   227.893\n",
      "    235.66164  ]]]]\n",
      "(4, 1, 96, 96)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape-mismatch for sum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2d8ba9dbb696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: Check forum again for answer by Umut, remove device param?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;31m# (1) start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# loss estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-38edaaeffb69>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/chainer/links/connection/convolution_2d.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m         return convolution_2d.convolution_2d(\n\u001b[1;32m    174\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             groups=self.groups)\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/chainer/functions/connection/convolution_2d.py\u001b[0m in \u001b[0;36mconvolution_2d\u001b[0;34m(x, W, b, stride, pad, cover_all, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_indexes_to_retain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_indexes_to_retain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Check for output array types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/chainer/functions/connection/convolution_2d.py\u001b[0m in \u001b[0;36mforward_cpu\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_grouped_convolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cpu_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_cpu_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/chainer/functions/connection/convolution_2d.py\u001b[0m in \u001b[0;36m_forward_cpu_core\u001b[0;34m(self, x, W, b)\u001b[0m\n\u001b[1;32m    108\u001b[0m             cover_all=self.cover_all, dy=self.dy, dx=self.dx)\n\u001b[1;32m    109\u001b[0m         y = numpy.tensordot(\n\u001b[0;32m--> 110\u001b[0;31m             col, W, ((1, 2, 3), (1, 2, 3))).astype(x.dtype, copy=False)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1281\u001b[0m                 \u001b[0maxes_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mndb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mequal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape-mismatch for sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;31m# Move the axes to sum over to the end of \"a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape-mismatch for sum"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_history['training'].append(0)\n",
    "\n",
    "    for j, batch in enumerate(training_iterator):\n",
    "        with chainer.using_config('train', True):\n",
    "            t, x = concat_examples(batch, device) # TODO: Check forum again for answer by Umut, remove device param?\n",
    "            y = model(x)\n",
    "            # (1) start\n",
    "            # loss estimation\n",
    "            loss = lossFunction(t, y)\n",
    "                \n",
    "            # backprop\n",
    "            model.cleargrads()\n",
    "            loss.backward()\n",
    "    \n",
    "            # parameter update\n",
    "            optimizer.update()\n",
    "            # (1) end\n",
    "\n",
    "        loss_history['training'][-1] += float(loss.data)\n",
    "        \n",
    "    print(j)\n",
    "\n",
    "    loss_history['training'][-1] /= j + 1\n",
    "    # (2) start\n",
    "    loss_history['validation'].append(0)\n",
    "\n",
    "    for j, batch in enumerate(validation_iterator):\n",
    "        with chainer.using_config('train', False):\n",
    "            t, x = concat_examples(batch, device)\n",
    "            y = model(x)\n",
    "            loss = lossFunction(t, y)\n",
    "\n",
    "        loss_history['validation'][-1] += float(loss.data)\n",
    "\n",
    "    loss_history['validation'][-1] /= j + 1\n",
    "    # (2) end\n",
    "    print('epoch: {:3d} / {:03d}, training loss: {:.4f}, validation loss: {:.4f}.'.format(epoch + 1, epochs, loss_history['training'][epoch-1], loss_history['validation'][epoch-1]))\n",
    "    np.savez('{:s}/loss_history_{:03d}.npz'.format(model_directory, epoch), loss_history)\n",
    "    # (3) start\n",
    "    save_npz('{:s}/model_{:03d}.npz'.format(model_directory, epoch), model)\n",
    "    # (3) end\n",
    "    save_npz('{:s}/optimizer_{:03d}.npz'.format(model_directory, epoch), optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YivB1PQ7Obh"
   },
   "source": [
    "**Test (45 points + 15 bonus points)**  \n",
    "In the following cell, you will test your model.  \n",
    "*Tasks*\n",
    "- (1) Estimate the test loss, print it and save it. (**15 points**)\n",
    "- (2) Estimate the validation metrics, print them and save them (tip: scikit-image) (**15 bonus points**)\n",
    "- (3) Plot example results (i.e., plot a few t, x and y) (**10 points**)\n",
    "- (4) Dicuss your implementation in 300 - 350 words (e.g., how good your results are, how you can improve your model, etc.) (**20 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zdlnCFDS-Cdh"
   },
   "outputs": [],
   "source": [
    "# (1), (2) and (3) start\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# (1), (2) and (3) end\n",
    "\n",
    "# (4) Write your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "weeks_2_and_3_assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
